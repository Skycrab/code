
# 1.利用hive计算

注意是否需要cube，对应排重字段需要使用cube(group by with cube)


# 2.数据存储
数据存储使用phoenix，等效于kylin用的hbase+sql解析
```
create table FBI.REPORT_DATA_100(
        report_id varchar not null,
        statdate varchar not null,
        dim3 varchar not null,
        dim4 varchar not null,
        dim5 varchar not null,
        dim6 varchar not null,
        dim7 varchar not null,
        dim8 varchar not null,
        dim9 varchar not null,
        dim10 varchar not null,
        dim11 varchar not null,
        dim12 varchar not null,
        dim13 varchar not null,
        dim14 varchar not null,
        dim15 varchar not null,
        dim16 varchar not null,
        dim17 varchar not null,
        dim18 varchar not null,
        dim19 varchar not null,
        dim20 varchar not null,
        valueArray double array,
        dimNameArray varchar array,
        constraint pk primary key (report_id,statdate,dim3,dim4,dim5,dim6,dim7,dim8 ,dim9,dim10,dim11,dim12,dim13,dim14,dim15,dim16,dim17,dim18,dim19,dim20)
)
DATA_BLOCK_ENCODING='FAST_DIFF',COMPRESSION = 'SNAPPY',SALT_BUCKETS = 8;

```

## hive导数到hbase

依赖

```
<dependency>
  <groupId>org.apache.phoenix</groupId>
  <artifactId>phoenix-core</artifactId>
  <version>4.11.0-HBase-0.98</version>
</dependency>
<dependency>
  <groupId>org.apache.phoenix</groupId>
  <artifactId>phoenix-spark</artifactId>
  <version>4.14.0-HBase-0.98</version>
  <exclusions>
    <exclusion>
      <groupId>org.apache.phoenix</groupId>
      <artifactId>phoenix-core</artifactId>
    </exclusion>
  </exclusions>
</dependency>
```

```
class ReportData extends Serializable {
  var report_id: String = _
  var statdate: String = _
  var dimArray: Array[String] = _
  var dimNameArray: Array[String] = _
  var valueArray: Array[Double] = _

  def toTable(): ReportTable = {

    val dims = Array.fill(18)(-10000)
    dimArray.copyToArray(dims)

    ReportTable(
      report_id,
      statdate,
      dims(0),
      dims(1),
      dims(2),
      dims(3),
      dims(4),
      dims(5),
      dims(6),
      dims(7),
      dims(8),
      dims(9),
      dims(10),
      dims(11),
      dims(12),
      dims(13),
      dims(14),
      dims(15),
      dims(16),
      dims(17),
      valueArray,
      dimNameArray
    )
  }
}


case class ReportTable(report_id: String,
                       statdate: String,
                       dim3: String,
                       dim4: String,
                       dim5: String,
                       dim6: String,
                       dim7: String,
                       dim8: String,
                       dim9: String,
                       dim10: String,
                       dim11: String,
                       dim12: String,
                       dim13: String,
                       dim14: String,
                       dim15: String,
                       dim16: String,
                       dim17: String,
                       dim18: String,
                       dim19: String,
                       dim20: String,
                       valueArray: Array[Double],
                       dimNameArray: Array[String]
                      )


val data = df.rdd.map(row => {
val reportData = new ReportData()
reportData.report_id = "100"
reportData.statdate = "2019-01-01"
val dimArray = List()
val dimNameArray = List()
val valueArray = List()

reportData.dimArray = dimArray
reportData.valueArray = valueArray
reportData.dimNameArray = dimNameArray
reportData.toTable()
}).toDF()
data.printSchema()
data.show(1)

import org.apache.phoenix.spark._
val conf = new Configuration
conf.set("mapreduce.output.fileoutputformat.outputdir", "/user//load_data/tmp")
data.saveToPhoenix(queryService.phoenixTableName, conf=conf, zkUrl=Option(LoadConfig.get("phoenix.jdbc.url")))

```


# 3.查询

查询cube报表

如查询115报表，时间2019-06-27到2019-06-21，dim6为1，其它维度'全部'(需要明确指定为-10000)， 按天查询

```
select 
    max(statdate)  as dim_item_name_96627,
    statdate as dim_item_code_96627,
    sum(valueArray[1])  as metric_name_96653,
    sum(valueArray[2])  as metric_name_96655,
    sum(valueArray[2])/sum(valueArray[1]) as metric_name_96663 
from FBI.REPORT_DATA_100 
where report_id = '115' and statdate<='2019-06-27' and statdate>='2019-06-21' 
and dim3='-10000' and dim6 in ('1') and dim5='-10000' 
and dim4='-10000' and dim8='-10000' and dim7='-10000'  
group by statdate
```

查询非cube报表

```
select 
  max(statdate)  as dim_item_name_98245,
  statdate as dim_item_code_98245,
  sum(valueArray[1])  as metric_name_98267,
  sum(valueArray[2])  as metric_name_98269 
from FBI.REPORT_DATA_100 
where report_id = '39' and statdate<='2019-06-27' and statdate>='2019-06-21' 
and dim5 in ('1')  
group by statdate
```
















